from datetime import datetime, timedelta
from django.db.models import Count, Q, Avg, Min, Max, Stddev
from django.db.models.functions import TruncHour
from django.utils import timezone
from rest_framework.decorators import api_view, permission_classes
from rest_framework import mixins, status, viewsets
from rest_framework.decorators import action
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response

from .models import RiskEvent, RobotComponent, RobotGroup, RobotAxisData
from .permissions import IsStaffOrReadOnly
from .serializers import RiskEventSerializer, RobotComponentSerializer, RobotGroupSerializer


class RobotGroupViewSet(mixins.ListModelMixin, viewsets.GenericViewSet):
    queryset = RobotGroup.objects.all()
    serializer_class = RobotGroupSerializer

    def list(self, request, *args, **kwargs):
        groups = list(self.get_queryset())
        for group in groups:
            qs = group.components.all()
            group._stats = {
                "total": qs.count(),
                "online": qs.filter(status="online").count(),
                "offline": qs.filter(status="offline").count(),
                "maintenance": qs.filter(status="maintenance").count(),
                "highRisk": qs.filter(level="H").count(),
                "historyHighRisk": qs.exclude(risk_history=[]).count(),
            }
        serializer = self.get_serializer(groups, many=True)
        return Response(serializer.data)


class RobotComponentViewSet(
    mixins.ListModelMixin,
    mixins.RetrieveModelMixin,
    mixins.UpdateModelMixin,
    viewsets.GenericViewSet,
):
    queryset = RobotComponent.objects.select_related("group").all()
    serializer_class = RobotComponentSerializer
    permission_classes = [IsStaffOrReadOnly]

    def get_queryset(self):
        qs = super().get_queryset()

        group_key = self.request.query_params.get("group")
        if group_key:
            qs = qs.filter(group__key=group_key)

        tab = self.request.query_params.get("tab")  # highRisk | all | history
        if tab == "highRisk":
            qs = qs.filter(level="H")
        elif tab == "history":
            qs = qs.exclude(risk_history=[])

        keyword = (self.request.query_params.get("keyword") or "").strip()
        if keyword:
            qs = qs.filter(
                Q(robot_id__icontains=keyword)
                | Q(name__icontains=keyword)
                | Q(part_no__icontains=keyword)
                | Q(reference_no__icontains=keyword)
                | Q(type_spec__icontains=keyword)
                | Q(tech__icontains=keyword)
            )

        status_filter = self.request.query_params.get("status")
        if status_filter:
            qs = qs.filter(status=status_filter)

        risk_filter = self.request.query_params.get("riskLevel")
        if risk_filter:
            qs = qs.filter(risk_level=risk_filter)

        level_filter = self.request.query_params.get("level")
        if level_filter:
            qs = qs.filter(level=level_filter)

        mark_mode = self.request.query_params.get("markMode")
        if mark_mode == "zero":
            qs = qs.filter(mark=0)
        elif mark_mode == "nonzero":
            qs = qs.exclude(mark=0)

        axis_keys_raw = (self.request.query_params.get("axisKeys") or "").strip()
        axis_keys = [k.strip() for k in axis_keys_raw.split(",") if k.strip()] if axis_keys_raw else []
        axis_key = (self.request.query_params.get("axisKey") or "").strip()
        if axis_key and axis_key not in axis_keys:
            axis_keys.append(axis_key)

        axis_ok = self.request.query_params.get("axisOk")
        allowed_axes = {"A1", "A2", "A3", "A4", "A5", "A6", "A7"}
        axis_keys = [k for k in axis_keys if k in allowed_axes]
        if axis_keys and axis_ok is not None:
            axis_ok_bool = str(axis_ok).lower() in {"1", "true", "yes"}
            if axis_ok_bool:
                for k in axis_keys:
                    qs = qs.filter(**{f"checks__{k}__ok": True})
            else:
                axis_q = Q()
                for k in axis_keys:
                    axis_q |= Q(**{f"checks__{k}__ok": False})
                qs = qs.filter(axis_q)

        return qs


class RiskEventViewSet(mixins.ListModelMixin, mixins.RetrieveModelMixin, viewsets.GenericViewSet):
    queryset = RiskEvent.objects.select_related("group").all()
    serializer_class = RiskEventSerializer

    def get_queryset(self):
        qs = super().get_queryset()
        group_key = self.request.query_params.get("group")
        if group_key:
            qs = qs.filter(group__key=group_key)

        status_filter = self.request.query_params.get("status")
        if status_filter:
            qs = qs.filter(status=status_filter)

        severity = self.request.query_params.get("severity")
        if severity:
            qs = qs.filter(severity=severity)

        return qs

    @action(detail=True, methods=["post"])
    def acknowledge(self, request, pk=None):
        event = self.get_object()
        event.status = "acknowledged"
        event.notes = request.data.get("notes", "") or event.notes
        event.save(update_fields=["status", "notes", "updated_at"])
        return Response(self.get_serializer(event).data)

    @action(detail=True, methods=["post"])
    def resolve(self, request, pk=None):
        event = self.get_object()
        event.status = "resolved"
        event.notes = request.data.get("notes", "") or event.notes
        event.save(update_fields=["status", "notes", "updated_at"])
        return Response(self.get_serializer(event).data)

    @action(detail=False, methods=["get"])
    def statistics(self, request):
        qs = self.get_queryset()
        severity_counts = qs.values("severity").annotate(count=Count("id"))
        status_counts = qs.values("status").annotate(count=Count("id"))

        severity_stats = {item["severity"]: item["count"] for item in severity_counts}
        total_stats = {item["status"]: item["count"] for item in status_counts}
        recent = qs.order_by("-triggered_at")[:5]

        return Response(
            {
                "severity_stats": severity_stats,
                "total_stats": total_stats,
                "recent_alerts": RiskEventSerializer(recent, many=True).data,
            }
        )


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def dashboard(request):
    now = timezone.now()
    since = now - timezone.timedelta(hours=24)
    high_risk_preview_limit = 12

    groups = list(RobotGroup.objects.all())
    group_payload = []
    for group in groups:
        qs = RobotComponent.objects.filter(group=group)
        high_risk_qs = (
            qs.filter(level="H")
            .order_by("-risk_score", "-updated_at")
            .values("id", "robot_id", "name")[:high_risk_preview_limit]
        )
        high_risk_devices = [
            {"id": item["id"], "robot_id": item["robot_id"], "name": item["name"] or item["robot_id"]}
            for item in high_risk_qs
        ]
        group_payload.append(
            {
                "key": group.key,
                "name": group.name,
                "expected_total": group.expected_total,
                "total": qs.count(),
                "highRisk": qs.filter(level="H").count(),
                "historyHighRisk": qs.exclude(risk_history=[]).count(),
                "marked": qs.exclude(mark=0).count(),
                "highRiskDevices": high_risk_devices,
                "highRiskDevicesPreviewLimit": high_risk_preview_limit,
            }
        )

    total = RobotComponent.objects.count()
    high_risk = RobotComponent.objects.filter(level="H").count()
    history_high_risk = RobotComponent.objects.exclude(risk_history=[]).count()
    marked = RobotComponent.objects.exclude(mark=0).count()

    level_dist = {item["level"]: item["count"] for item in RobotComponent.objects.values("level").annotate(count=Count("id"))}

    axes = ["A1", "A2", "A3", "A4", "A5", "A6", "A7"]
    axis_bad = {}
    for axis in axes:
        axis_bad[axis] = RobotComponent.objects.filter(**{f"checks__{axis}__ok": False}).count()

    event_qs = RiskEvent.objects.filter(triggered_at__gte=since, triggered_at__lte=now)
    hourly = (
        event_qs.annotate(hour=TruncHour("triggered_at"))
        .values("hour")
        .annotate(count=Count("id"))
        .order_by("hour")
    )
    hourly_series = [{"time": item["hour"].isoformat(), "count": item["count"]} for item in hourly]

    recent_components = RobotComponent.objects.select_related("group").order_by("-updated_at")[:20]
    recent_payload = RobotComponentSerializer(recent_components, many=True).data

    top_high_risk = RobotComponent.objects.select_related("group").filter(level="H").order_by("-updated_at")[:20]
    top_high_risk_payload = RobotComponentSerializer(top_high_risk, many=True).data

    return Response(
        {
            "summary": {
                "total": total,
                "highRisk": high_risk,
                "historyHighRisk": history_high_risk,
                "marked": marked,
            },
            "groupStats": group_payload,
            "levelDistribution": level_dist,
            "axisBad": axis_bad,
            "events24h": hourly_series,
            "recentUpdated": recent_payload,
            "highRiskList": top_high_risk_payload,
            "generatedAt": now.isoformat(),
        }
    )


@api_view(["GET"])
@permission_classes([IsAuthenticated])
def axis_data(request):
    """
    Get axis time series and aggregated data for a robot
    Query parameters:
    - part_no: Robot part number (e.g., 'AS33_020RB_400')
    - axis: Axis key (A1-A7)
    - start_time: Optional start time (default: 30 days ago)
    - end_time: Optional end time (default: now)
    """
    part_no = request.query_params.get("part_no", "").upper()
    axis = request.query_params.get("axis", "A1")

    if not part_no:
        return Response({"error": "part_no is required"}, status=400)

    # Map part_no to table name
    table_name = part_no.lower()

    # Default time range: last 30 days (increased from 7 days)
    end_time = request.query_params.get("end_time")
    start_time = request.query_params.get("start_time")

    if not end_time:
        end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    if not start_time:
        start_time = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d %H:%M:%S')

    try:
        # Connect to MySQL and fetch data
        conn = pymysql.connect(**ROBOT_DB_CONFIG)

        try:
            # First, check if we have data in the specified range
            check_query = f"SELECT COUNT(*) as cnt FROM `{table_name}` WHERE `Timestamp` BETWEEN %s AND %s"

            # Fetch time series data
            query = f"SELECT * FROM `{table_name}` WHERE `Timestamp` BETWEEN %s AND %s ORDER BY `Timestamp`"
            df = pd.read_sql(query, conn, params=(start_time, end_time))

            # If no data in specified range, try to get the most recent data
            if df.empty:
                # Get the latest available data (last 1000 records)
                query_recent = f"SELECT * FROM `{table_name}` ORDER BY `Timestamp` DESC LIMIT 1000"
                df = pd.read_sql(query_recent, conn)

                if df.empty:
                    return Response({"error": "No data found", "data": None, "aggregated": None})

                # Sort by timestamp for processing
                df = df.sort_values(by='Timestamp')

                # Update time range for response
                if not df.empty:
                    start_time = df['Timestamp'].min()
                    end_time = df['Timestamp'].max()

            if df.empty:
                return Response({"error": "No data found", "data": None, "aggregated": None})

            # Process data
            # Drop unnecessary columns
            cols_to_drop = ['A1_marker', 'A2_marker', 'A3_marker', 'A4_marker',
                            'A5_marker', 'A6_marker', 'A7_marker', 'SUB']
            for col in cols_to_drop:
                if col in df.columns:
                    del df[col]

            # Remove duplicates
            df = df.drop_duplicates()

            # Convert time
            df['Time'] = pd.to_datetime(df['Timestamp']) + timedelta(hours=8)
            df['Timestamp'] = df['Time'].astype(str)

            # Convert data types
            df['SNR_C'] = df['SNR_C'].astype(int)
            for i in range(1, 8):
                col = f'AxisP{i}'
                if col in df.columns:
                    df[col] = df[col].astype(float)

            # Sort data
            deft = df.sort_values(by=['SNR_C', 'Time'])
            deft['sort'] = range(1, len(deft) + 1)

            # Calculate aggregated data (quantiles)
            ref_cols = [f'MAXCurr_A{i}' for i in range(1, 7)] + ['MAXCurr_E1'] + \
                       [f'MinCurr_A{i}' for i in range(1, 7)] + ['MinCurr_E1']
            ref = deft.groupby('SNR_C')[[c for c in ref_cols if c in deft.columns]].last()

            x_tex = deft["SNR_C"].sort_values(ascending=True).unique().astype(str)

            # Lower quantile (1%)
            curr_cols = [f'Curr_A{i}' for i in range(1, 7)] + ['Curr_E1']
            lq_cols = [c for c in curr_cols if c in deft.columns]
            LQ = deft.groupby('SNR_C')[lq_cols].quantile(q=0.01, interpolation='nearest')
            LQ.columns = [f'{col}_LQ' for col in LQ.columns]

            # Upper quantile (99%)
            HQ = deft.groupby('SNR_C')[lq_cols].quantile(q=0.99, interpolation='nearest')
            HQ.columns = [f'{col}_HQ' for col in HQ.columns]

            # Labels
            labeltext = deft.groupby('SNR_C')['P_name'].last()

            # Merge all
            Q = pd.merge(pd.merge(pd.merge(LQ, HQ, left_index=True, right_index=True, how='outer'),
                                  ref, left_index=True, right_index=True, how='inner'),
                         labeltext, left_index=True, right_index=True, how='inner').reset_index()
            Q["SNR_C"] = x_tex

            # Convert to dict for JSON response
            response_data = {
                'table_name': table_name,
                'axis': axis,
                'start_time': start_time,
                'end_time': end_time,
                'data': deft.to_dict('records'),
                'aggregated': Q.to_dict('records'),
                'total_records': len(deft)
            }

            return Response(response_data)

        finally:
            conn.close()

    except Exception as e:
        return Response({"error": str(e)}, status=500)
